{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da559cc",
   "metadata": {},
   "source": [
    "<img src='images/besm.png' width='150px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f12da2f",
   "metadata": {},
   "source": [
    "<center> درس پردازش زبان‌های طبیعی </center>\n",
    "<center> آزمایشگاه پردازش هوشمند متن و زبان و علوم انسانی محاسباتی </center>\n",
    "<br>\n",
    "<center> http://language.ml </center>\n",
    "<center> contact: asgari [AT] berkeley [dot] edu </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39afbfbd",
   "metadata": {},
   "source": [
    "</h1> <h1 style='direction:rtl'>انگلیسی - پیش پردازش و بررسی متن \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a586a14",
   "metadata": {},
   "source": [
    "</h2> <h2 style='direction:rtl;'>یک پاراگراف توضیح در مورد متنی که انتخاب کرده اید: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4d480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dde68e0f",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § مارک‌آپ  </h2> \n",
    "\n",
    "<p style='direction:rtl;' >\n",
    "برای یادگیری بیشتر به این <a href='https://docs.github.com/en/github/writing-on-github/getting-started-with-writing-and-formatting-on-github'> لینک  </a> مراجعه کنید.\n",
    "        \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ebde3",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § نمونه‌ای از متن  </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79245418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import tqdm\n",
    "import nltk, re\n",
    "from nltk.corpus import brown\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "all_categories = brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3079af03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647e2a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_brown_sentences_religion = list(brown.sents(categories=['religion']))\n",
    "all_brown_sentences_news = list(brown.sents(categories=['news']))\n",
    "\n",
    "all_brown_tagged_sentences = list(brown.tagged_sents(categories=all_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1407e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it also contains nearly all the mass , and the atomic energy .\n",
      "This problem is illustrated by the fact that many local churches drop from the active membership rolls each year as many as they receive into the fellowship .\n",
      "everyone is to be a participant .\n",
      "Even more of an obstacle is the difficulty of separating the influence of Christianity from other factors .\n",
      "Reproach the Catholics for it ! !\n",
      "`` If Philip Toynbee is claiming that the choice lies between capitulation and the risk of nuclear war , I think he is right .\n",
      "The Mahayana that developed in the north was a religion of idolatry and coarse magic , that made the world into a huge magical garden .\n",
      "But the enemy answers them from loudspeakers that pour out Communist propaganda with a generous mixture of terrible profanity .\n",
      "Do not wonder that I said to thee , `` You must be born again '' .\n",
      "Finally men arrested Him , gave Him a mock trial , flogged Him , nailed Him on a cross and hung Him between earth and heaven ; ;\n",
      "1 )\n",
      "So it is spiritually .\n",
      "In fact , He came into this world Himself , in the person of His Son , Jesus Christ , who stood here amid the darkness of human sin and said : `` I am the light of the world : he that followeth me shall not walk in darkness , but shall have the light of life '' .\n",
      "Many of you are familiar , I'm sure , with the story of my early struggles : the fire in January , 1947 , that destroyed everything -- even our precious list of subscribers .\n",
      "Living in urban conditions , away from the deadweight of village constraint and the constrictions of a thatched-roof world view , the individual may find it possible , say , to commit adultery not only without personal misgivings , but also without suffering any adverse effects in his worldly fortunes .\n",
      "If you know that he ( God ) is just ( righteous ) , know that everyone also who does what is just ( righteous ) has been born of him .\n",
      "In the supernatural atmosphere of cosmic government , only the ruling elite was ever concerned with a kingdom-wide ordering of nature : popular religion aimed at more personal benefits from magical powers .\n",
      "Garrison , Massachusetts born of Nova Scotian parentage , was by temperament and conviction a reformer .\n",
      "Recently , a group of the faculty at Wesleyan University's Public Affairs Center sought some answers to these questions .\n",
      "Science is placing in our hands the ultimate power of the universe , the power of the atom .\n",
      "5 )\n",
      "Though the reference to race was stricken by the association in 1950 , being an agent of such `` detrimental '' influences still appears as the cardinal sin realtors see themselves committed to avoid .\n",
      "I knew in that moment that I did not have any choice .\n",
      "it has sought new sympathy for the human aspirations of racial minority groups in this country .\n",
      "Isn't that where most of them are already -- right out on the front page of our newspapers ? ?\n",
      "When that fear has been removed by faith in Jesus Christ , when we know that He is our Savior , that He has paid our debt with His blood , that He has met the demands of God's justice and thus has turned His wrath away -- when we know that , we have peace with God in our hearts ; ;\n",
      "Every detail of the service merits careful attention -- the hymns , the sermon , the ritual , the right hand of fellowship , the introduction to the congregation , the welcome of the congregation .\n",
      "Sidney Fields , years of experience as a New York columnist ; ;\n",
      "Arthur Gordon comes once a month all the way from Georgia .\n",
      "It can mean the difference between participation and inaction , spiritual growth and decay .\n"
     ]
    }
   ],
   "source": [
    "for x in random.sample(all_brown_sentences_religion, 30):\n",
    "    print(' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb4a842",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § نرمالایز کردن  </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "723bc839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentence(tokenized_sents, minimum_length=2, stopword_removal=True, stopwords_domain=[], lower_case=False, punctuation_removal=True):\n",
    "    '''\n",
    "    normalization function\n",
    "    '''\n",
    "    normalized_sents = tokenized_sents\n",
    "    \n",
    "    if stopword_removal:\n",
    "        # Remove stopwords in English and also the given domain stopwords\n",
    "        stopwords = [x.lower() for x in nltk.corpus.stopwords.words('english')]\n",
    "        normalized_sents=[[word for word in sentence if (word.lower() not in stopwords_domain + stopwords)] for sentence in tokenized_sents ]\n",
    "\n",
    "    if punctuation_removal:\n",
    "        # Remove punctuations\n",
    "        normalized_sents=[[word for word in sentence if word not in string.punctuation] for sentence in normalized_sents ]\n",
    "\n",
    "    if lower_case:\n",
    "        # Convert everything to lowercase and filter based on a min length\n",
    "        normalized_sents=[[word.lower() for word in sentence if len(word)>minimum_length] for sentence in normalized_sents ]\n",
    "\n",
    "    elif minimum_length>1:\n",
    "        normalized_sents= [[word for word in sentence if len(word)>minimum_length] for sentence in normalized_sents ]        \n",
    "        \n",
    "    return normalized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca6e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_sentences_religion = normalize_sentence(all_brown_sentences_religion)\n",
    "normalized_sentences_news = normalize_sentence(all_brown_sentences_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a63d97e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'investigation', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', 'evidence', 'irregularities', 'took', 'place']\n",
      "\n",
      "['jury', 'said', 'term-end', 'presentments', 'City', 'Executive', 'Committee', 'over-all', 'charge', 'election', 'deserves', 'praise', 'thanks', 'City', 'Atlanta', 'manner', 'election', 'conducted']\n",
      "\n",
      "['September-October', 'term', 'jury', 'charged', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'investigate', 'reports', 'possible', 'irregularities', 'hard-fought', 'primary', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.']\n",
      "\n",
      "['result', 'although', 'still', 'make', 'use', 'distinction', 'much', 'confusion', 'meaning', 'basic', 'terms', 'employed']\n",
      "\n",
      "['meant', 'spirit', 'matter']\n",
      "\n",
      "['terms', 'generally', 'taken', 'granted', 'though', 'referred', 'direct', 'axiomatic', 'elements', 'common', 'experience']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in normalized_sentences_news[0:3] + normalized_sentences_religion[0:3]:\n",
    "    print(x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42518dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word             Frequency        % of the total  \n",
      "God              131              7.634032634032634\n",
      "world            90               5.244755244755245\n",
      "one              87               5.06993006993007\n",
      "may              78               4.545454545454546\n",
      "new              77               4.487179487179487\n",
      "would            68               3.9627039627039626\n",
      "man              64               3.7296037296037294\n",
      "could            59               3.4382284382284385\n",
      "Christ           59               3.4382284382284385\n",
      "also             56               3.263403263403263\n",
      "life             55               3.205128205128205\n",
      "must             54               3.146853146853147\n",
      "church           51               2.972027972027972\n",
      "Christian        50               2.9137529137529135\n",
      "power            49               2.8554778554778557\n",
      "members          49               2.8554778554778557\n",
      "spirit           46               2.6806526806526807\n",
      "many             46               2.6806526806526807\n",
      "human            43               2.505827505827506\n",
      "Church           43               2.505827505827506\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "mp_freqdist = FreqDist(itertools.chain(*normalized_sentences_religion))               # compute the frequency distribution\n",
    "top20words=mp_freqdist.most_common(20)      # show the top 20 (word, frequency) pairs\n",
    "print ('%-16s' % 'word', '%-16s' % 'Frequency','%-16s' %  '% of the total')\n",
    "for topword in top20words:\n",
    "    percent=(topword[1]/len(normalized_sentences_religion))*100\n",
    "    print ('%-16s' % topword[0], '%-16s' % topword[1],'%-16s' %  percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2578335",
   "metadata": {},
   "source": [
    "# Religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd4b7ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words  17145           \n",
      "Number of unique words 6081            \n",
      "Average word length 6.614289880431612\n",
      "Average sentence length in characters 12.228579760863225\n",
      "Longest word     Preparation-Inquirers'\n"
     ]
    }
   ],
   "source": [
    "sentence_words = list(itertools.chain(*normalized_sentences_religion))\n",
    "print ('%-16s' % 'Number of words', '%-16s' % len(sentence_words))\n",
    "print ('%-16s' % 'Number of unique words', '%-16s' % len(set(sentence_words)))\n",
    "avg=np.sum([len(word) for word in sentence_words])/len(sentence_words)\n",
    "print ('%-16s' % 'Average word length', '%-16s' % avg)\n",
    "avg_sentence_length_c=np.mean([len(' '.join(sentence)) for sentence in sentence_words])\n",
    "print ('%-16s' % 'Average sentence length in characters', '%-16s' % avg_sentence_length_c)\n",
    "print ('%-16s' % 'Longest word', '%-16s' % sentence_words[np.argmax([len(word) for word in sentence_words])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe2a2d",
   "metadata": {},
   "source": [
    "# News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e658684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words  50351           \n",
      "Number of unique words 13970           \n",
      "Average word length 6.407380191058768\n",
      "Average sentence length in characters 11.814760382117536\n",
      "Longest word     Scotch-Irish-Scandinavian\n"
     ]
    }
   ],
   "source": [
    "sentence_words = list(itertools.chain(*normalized_sentences_news))\n",
    "print ('%-16s' % 'Number of words', '%-16s' % len(sentence_words))\n",
    "print ('%-16s' % 'Number of unique words', '%-16s' % len(set(sentence_words)))\n",
    "avg=np.sum([len(word) for word in sentence_words])/len(sentence_words)\n",
    "print ('%-16s' % 'Average word length', '%-16s' % avg)\n",
    "avg_sentence_length_c=np.mean([len(' '.join(sentence)) for sentence in sentence_words])\n",
    "print ('%-16s' % 'Average sentence length in characters', '%-16s' % avg_sentence_length_c)\n",
    "print ('%-16s' % 'Longest word', '%-16s' % sentence_words[np.argmax([len(word) for word in sentence_words])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e6e6a",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § توکنایزیشن  </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac1feb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'delta',\n",
       " 'variant',\n",
       " 'of',\n",
       " 'COVID-19',\n",
       " 'is',\n",
       " 'not',\n",
       " 'done',\n",
       " 'with',\n",
       " 'the',\n",
       " 'U.S.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample\n",
    "\n",
    "text = 'The delta variant of COVID-19 is not done with the U.S.'\n",
    "\n",
    "# http://stackoverflow.com/questions/36353125/nltk-regular-expression-tokenizer\n",
    "pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\$?\\d+(?:\\.\\d+)?%?\\s?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''\n",
    "\n",
    "pattern = re.compile(pattern)\n",
    "\n",
    "tokenized_abstracts=nltk.regexp_tokenize(text, pattern)\n",
    "tokenized_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca68968d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['result',\n",
       "  'although',\n",
       "  'still',\n",
       "  'make',\n",
       "  'use',\n",
       "  'distinction',\n",
       "  'much',\n",
       "  'confusion',\n",
       "  'meaning',\n",
       "  'basic',\n",
       "  'terms',\n",
       "  'employed'],\n",
       " ['meant', 'spirit', 'matter'],\n",
       " ['terms',\n",
       "  'generally',\n",
       "  'taken',\n",
       "  'granted',\n",
       "  'though',\n",
       "  'referred',\n",
       "  'direct',\n",
       "  'axiomatic',\n",
       "  'elements',\n",
       "  'common',\n",
       "  'experience'],\n",
       " ['Yet', 'contemporary', 'context', 'precisely', 'one', 'must'],\n",
       " ['modern',\n",
       "  'world',\n",
       "  'neither',\n",
       "  'spirit',\n",
       "  'matter',\n",
       "  'refer',\n",
       "  'generally',\n",
       "  'agreed-upon',\n",
       "  'elements',\n",
       "  'experience'],\n",
       " ['transitional',\n",
       "  'stage',\n",
       "  'many',\n",
       "  'connotations',\n",
       "  'former',\n",
       "  'usage',\n",
       "  'revised',\n",
       "  'rejected'],\n",
       " ['words',\n",
       "  'used',\n",
       "  'never',\n",
       "  'sure',\n",
       "  'traditional',\n",
       "  'meanings',\n",
       "  'user',\n",
       "  'may',\n",
       "  'mind',\n",
       "  'extent',\n",
       "  'revisions',\n",
       "  'rejections',\n",
       "  'former',\n",
       "  'understandings',\n",
       "  'correspond'],\n",
       " ['One',\n",
       "  'widespread',\n",
       "  'features',\n",
       "  'contemporary',\n",
       "  'thought',\n",
       "  'almost',\n",
       "  'universal',\n",
       "  'disbelief',\n",
       "  'reality',\n",
       "  'spirit'],\n",
       " ['centuries',\n",
       "  'ago',\n",
       "  'world',\n",
       "  'spirits',\n",
       "  'populous',\n",
       "  'real',\n",
       "  'world',\n",
       "  'material',\n",
       "  'entities'],\n",
       " ['popular', 'thought', 'highly', 'educated', 'well', 'true']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_sentences_religion[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf8eb3",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § جمله‌بندی  </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4daaa0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Tom, how are you?\n",
      "I’m fine, thanks.\n",
      "How are you?\n",
      "Great!\n",
      "thank you.\n",
      "Hi Tom, how are you?\n",
      "I’m fine, thanks.\n",
      "How are you?\n",
      "Great!\n",
      "thank you.\n"
     ]
    }
   ],
   "source": [
    "# Example in NLTK\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text=\"\"\"Hi Tom, how are you? I’m fine, thanks. How are you? Great! thank you.\"\"\"\n",
    "\n",
    "sents=sent_tokenize(text)\n",
    "for x in sents:\n",
    "    print(x)\n",
    "\n",
    "# Explore Spacy\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') # or whatever model you have installed\n",
    "doc = nlp(text)\n",
    "sents = [sent for sent in doc.sents]\n",
    "for x in sents:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e11d11c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<h2 style='direction:rtl;'> § تحلیل بسامد  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b79f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "all_tokens_religion = list(itertools.chain(*normalized_sentences_religion))\n",
    "all_tokens_news = list(itertools.chain(*normalized_sentences_news))\n",
    "\n",
    "\n",
    "dataframe = {}\n",
    "\n",
    "for opt in ['religion', 'news']:\n",
    "     dataframe[opt] = FreqDist(eval(F\"all_tokens_{opt}\")).most_common(50)\n",
    "\n",
    "freq_analysis = pd.DataFrame(dataframe)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f4cfe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>religion</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(God, 131)</td>\n",
       "      <td>(said, 402)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(world, 90)</td>\n",
       "      <td>(Mrs., 253)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(one, 87)</td>\n",
       "      <td>(would, 244)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(may, 78)</td>\n",
       "      <td>(one, 184)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(new, 77)</td>\n",
       "      <td>(Mr., 170)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(would, 68)</td>\n",
       "      <td>(last, 161)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(man, 64)</td>\n",
       "      <td>(two, 157)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(could, 59)</td>\n",
       "      <td>(new, 148)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Christ, 59)</td>\n",
       "      <td>(first, 143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(also, 56)</td>\n",
       "      <td>(year, 138)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(life, 55)</td>\n",
       "      <td>(home, 127)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(must, 54)</td>\n",
       "      <td>(also, 120)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(church, 51)</td>\n",
       "      <td>(made, 107)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(Christian, 50)</td>\n",
       "      <td>(years, 102)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(power, 49)</td>\n",
       "      <td>(time, 97)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(members, 49)</td>\n",
       "      <td>(three, 97)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(spirit, 46)</td>\n",
       "      <td>(New, 93)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(many, 46)</td>\n",
       "      <td>(state, 90)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(human, 43)</td>\n",
       "      <td>(President, 89)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(Church, 43)</td>\n",
       "      <td>(week, 86)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(death, 41)</td>\n",
       "      <td>(could, 86)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(faith, 40)</td>\n",
       "      <td>(four, 73)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(say, 39)</td>\n",
       "      <td>(man, 72)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(even, 39)</td>\n",
       "      <td>(House, 71)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(men, 39)</td>\n",
       "      <td>(back, 69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(good, 37)</td>\n",
       "      <td>(members, 69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(years, 37)</td>\n",
       "      <td>(American, 67)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(England, 37)</td>\n",
       "      <td>(may, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(still, 35)</td>\n",
       "      <td>(program, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(see, 35)</td>\n",
       "      <td>(work, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(time, 34)</td>\n",
       "      <td>(get, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(people, 34)</td>\n",
       "      <td>(Kennedy, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(Catholic, 34)</td>\n",
       "      <td>(John, 65)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(Jesus, 33)</td>\n",
       "      <td>(school, 65)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(number, 33)</td>\n",
       "      <td>(night, 64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(membership, 33)</td>\n",
       "      <td>(State, 63)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(born, 33)</td>\n",
       "      <td>(meeting, 62)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(way, 31)</td>\n",
       "      <td>(since, 61)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(know, 30)</td>\n",
       "      <td>(per, 61)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(John, 30)</td>\n",
       "      <td>(day, 61)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(sin, 30)</td>\n",
       "      <td>(many, 60)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(action, 30)</td>\n",
       "      <td>(U.S., 58)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(churches, 29)</td>\n",
       "      <td>(United, 58)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(much, 28)</td>\n",
       "      <td>(yesterday, 56)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(made, 28)</td>\n",
       "      <td>(Monday, 54)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(St., 28)</td>\n",
       "      <td>(tax, 53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(Parker, 28)</td>\n",
       "      <td>(told, 53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(experience, 27)</td>\n",
       "      <td>(president, 53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>(two, 27)</td>\n",
       "      <td>(even, 53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>(said, 27)</td>\n",
       "      <td>(administration, 52)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            religion                  news\n",
       "0         (God, 131)           (said, 402)\n",
       "1        (world, 90)           (Mrs., 253)\n",
       "2          (one, 87)          (would, 244)\n",
       "3          (may, 78)            (one, 184)\n",
       "4          (new, 77)            (Mr., 170)\n",
       "5        (would, 68)           (last, 161)\n",
       "6          (man, 64)            (two, 157)\n",
       "7        (could, 59)            (new, 148)\n",
       "8       (Christ, 59)          (first, 143)\n",
       "9         (also, 56)           (year, 138)\n",
       "10        (life, 55)           (home, 127)\n",
       "11        (must, 54)           (also, 120)\n",
       "12      (church, 51)           (made, 107)\n",
       "13   (Christian, 50)          (years, 102)\n",
       "14       (power, 49)            (time, 97)\n",
       "15     (members, 49)           (three, 97)\n",
       "16      (spirit, 46)             (New, 93)\n",
       "17        (many, 46)           (state, 90)\n",
       "18       (human, 43)       (President, 89)\n",
       "19      (Church, 43)            (week, 86)\n",
       "20       (death, 41)           (could, 86)\n",
       "21       (faith, 40)            (four, 73)\n",
       "22         (say, 39)             (man, 72)\n",
       "23        (even, 39)           (House, 71)\n",
       "24         (men, 39)            (back, 69)\n",
       "25        (good, 37)         (members, 69)\n",
       "26       (years, 37)        (American, 67)\n",
       "27     (England, 37)             (may, 66)\n",
       "28       (still, 35)         (program, 66)\n",
       "29         (see, 35)            (work, 66)\n",
       "30        (time, 34)             (get, 66)\n",
       "31      (people, 34)         (Kennedy, 66)\n",
       "32    (Catholic, 34)            (John, 65)\n",
       "33       (Jesus, 33)          (school, 65)\n",
       "34      (number, 33)           (night, 64)\n",
       "35  (membership, 33)           (State, 63)\n",
       "36        (born, 33)         (meeting, 62)\n",
       "37         (way, 31)           (since, 61)\n",
       "38        (know, 30)             (per, 61)\n",
       "39        (John, 30)             (day, 61)\n",
       "40         (sin, 30)            (many, 60)\n",
       "41      (action, 30)            (U.S., 58)\n",
       "42    (churches, 29)          (United, 58)\n",
       "43        (much, 28)       (yesterday, 56)\n",
       "44        (made, 28)          (Monday, 54)\n",
       "45         (St., 28)             (tax, 53)\n",
       "46      (Parker, 28)            (told, 53)\n",
       "47  (experience, 27)       (president, 53)\n",
       "48         (two, 27)            (even, 53)\n",
       "49        (said, 27)  (administration, 52)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073ccc77",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<h2 style='direction:rtl;'> § استفاده از lemmatization, stemming  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d8854a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer  = PorterStemmer()\n",
    "\n",
    "\n",
    "def get_lemma_set(tok, opt=1):\n",
    "    if opt ==1:\n",
    "        return stemmer.stem(tok)\n",
    "    if opt ==2:\n",
    "        return lemmatizer.lemmatize(tok)\n",
    "    if opt ==3:\n",
    "        # write your own\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9290ab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17145/17145 [00:01<00:00, 14511.26it/s]\n",
      "100%|██████████| 50351/50351 [00:00<00:00, 341028.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>religion</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(God, 131)</td>\n",
       "      <td>(said, 402)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(one, 93)</td>\n",
       "      <td>(Mrs., 253)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(world, 91)</td>\n",
       "      <td>(would, 244)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(church, 80)</td>\n",
       "      <td>(year, 240)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(may, 78)</td>\n",
       "      <td>(one, 192)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(new, 77)</td>\n",
       "      <td>(Mr., 170)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(would, 68)</td>\n",
       "      <td>(last, 161)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(man, 64)</td>\n",
       "      <td>(two, 157)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(life, 62)</td>\n",
       "      <td>(new, 148)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(power, 60)</td>\n",
       "      <td>(first, 143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(could, 59)</td>\n",
       "      <td>(home, 136)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(member, 59)</td>\n",
       "      <td>(also, 120)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(Christ, 59)</td>\n",
       "      <td>(time, 115)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(spirit, 58)</td>\n",
       "      <td>(state, 112)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(also, 56)</td>\n",
       "      <td>(week, 111)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(must, 54)</td>\n",
       "      <td>(made, 107)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(number, 54)</td>\n",
       "      <td>(member, 104)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(Christian, 50)</td>\n",
       "      <td>(school, 102)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(say, 48)</td>\n",
       "      <td>(day, 99)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(many, 46)</td>\n",
       "      <td>(three, 97)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(year, 45)</td>\n",
       "      <td>(New, 93)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(human, 44)</td>\n",
       "      <td>(President, 89)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(Church, 43)</td>\n",
       "      <td>(could, 86)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(way, 42)</td>\n",
       "      <td>(program, 78)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(mean, 42)</td>\n",
       "      <td>(work, 74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(death, 41)</td>\n",
       "      <td>(get, 74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(thing, 40)</td>\n",
       "      <td>(month, 74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(faith, 40)</td>\n",
       "      <td>(four, 73)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(time, 39)</td>\n",
       "      <td>(game, 73)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(even, 39)</td>\n",
       "      <td>(man, 72)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(men, 39)</td>\n",
       "      <td>(House, 71)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(see, 38)</td>\n",
       "      <td>(back, 70)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(good, 37)</td>\n",
       "      <td>(law, 68)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(England, 37)</td>\n",
       "      <td>(car, 68)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(people, 36)</td>\n",
       "      <td>(plan, 67)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(still, 35)</td>\n",
       "      <td>(meeting, 67)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(know, 35)</td>\n",
       "      <td>(American, 67)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(sin, 35)</td>\n",
       "      <td>(may, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(fear, 34)</td>\n",
       "      <td>(Kennedy, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(need, 34)</td>\n",
       "      <td>(night, 65)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(Catholic, 34)</td>\n",
       "      <td>(run, 65)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(make, 33)</td>\n",
       "      <td>(John, 65)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(word, 33)</td>\n",
       "      <td>(State, 63)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(Jesus, 33)</td>\n",
       "      <td>(tax, 63)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(membership, 33)</td>\n",
       "      <td>(since, 61)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(born, 33)</td>\n",
       "      <td>(per, 61)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(work, 32)</td>\n",
       "      <td>(bill, 61)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(atom, 32)</td>\n",
       "      <td>(many, 60)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>(experience, 31)</td>\n",
       "      <td>(company, 60)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>(John, 30)</td>\n",
       "      <td>(sale, 59)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            religion             news\n",
       "0         (God, 131)      (said, 402)\n",
       "1          (one, 93)      (Mrs., 253)\n",
       "2        (world, 91)     (would, 244)\n",
       "3       (church, 80)      (year, 240)\n",
       "4          (may, 78)       (one, 192)\n",
       "5          (new, 77)       (Mr., 170)\n",
       "6        (would, 68)      (last, 161)\n",
       "7          (man, 64)       (two, 157)\n",
       "8         (life, 62)       (new, 148)\n",
       "9        (power, 60)     (first, 143)\n",
       "10       (could, 59)      (home, 136)\n",
       "11      (member, 59)      (also, 120)\n",
       "12      (Christ, 59)      (time, 115)\n",
       "13      (spirit, 58)     (state, 112)\n",
       "14        (also, 56)      (week, 111)\n",
       "15        (must, 54)      (made, 107)\n",
       "16      (number, 54)    (member, 104)\n",
       "17   (Christian, 50)    (school, 102)\n",
       "18         (say, 48)        (day, 99)\n",
       "19        (many, 46)      (three, 97)\n",
       "20        (year, 45)        (New, 93)\n",
       "21       (human, 44)  (President, 89)\n",
       "22      (Church, 43)      (could, 86)\n",
       "23         (way, 42)    (program, 78)\n",
       "24        (mean, 42)       (work, 74)\n",
       "25       (death, 41)        (get, 74)\n",
       "26       (thing, 40)      (month, 74)\n",
       "27       (faith, 40)       (four, 73)\n",
       "28        (time, 39)       (game, 73)\n",
       "29        (even, 39)        (man, 72)\n",
       "30         (men, 39)      (House, 71)\n",
       "31         (see, 38)       (back, 70)\n",
       "32        (good, 37)        (law, 68)\n",
       "33     (England, 37)        (car, 68)\n",
       "34      (people, 36)       (plan, 67)\n",
       "35       (still, 35)    (meeting, 67)\n",
       "36        (know, 35)   (American, 67)\n",
       "37         (sin, 35)        (may, 66)\n",
       "38        (fear, 34)    (Kennedy, 66)\n",
       "39        (need, 34)      (night, 65)\n",
       "40    (Catholic, 34)        (run, 65)\n",
       "41        (make, 33)       (John, 65)\n",
       "42        (word, 33)      (State, 63)\n",
       "43       (Jesus, 33)        (tax, 63)\n",
       "44  (membership, 33)      (since, 61)\n",
       "45        (born, 33)        (per, 61)\n",
       "46        (work, 32)       (bill, 61)\n",
       "47        (atom, 32)       (many, 60)\n",
       "48  (experience, 31)    (company, 60)\n",
       "49        (John, 30)       (sale, 59)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = 2\n",
    "\n",
    "religion_tokens_nonstop_lemstem =   [get_lemma_set(t, opt) for t in tqdm.tqdm(all_tokens_religion)]\n",
    "news_tokens_nonstop_lemstem = [get_lemma_set(t, opt) for t in tqdm.tqdm(all_tokens_news)]\n",
    "\n",
    "dataframe_nonstop_lemstem = {}\n",
    "\n",
    "for opt in ['religion', 'news']:\n",
    "     dataframe_nonstop_lemstem[opt] = FreqDist(eval(F\"{opt}_tokens_nonstop_lemstem\")).most_common(50)\n",
    "\n",
    "freq_analysis_nonstop_lemstem = pd.DataFrame(dataframe_nonstop_lemstem)   \n",
    "freq_analysis_nonstop_lemstem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe821bc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<h2 style='direction:rtl;'> § استفاده از POS-tags  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "318f56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f03a8a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_sets(sentences):\n",
    "    size = int(len(sentences) * 0.9)\n",
    "    train_sents = sentences[:size]\n",
    "    test_sents = sentences[size:]\n",
    "    return train_sents, test_sents\n",
    "\n",
    "def build_backoff_tagger (train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "\n",
    "\n",
    "def train_tagger(already_tagged_sents):\n",
    "    train_sents, test_sents = create_data_sets(already_tagged_sents)\n",
    "    ngram_tagger = build_backoff_tagger(train_sents)\n",
    "    print (\"%0.3f pos accuracy on test set\" % ngram_tagger.evaluate(test_sents))\n",
    "    return ngram_tagger    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df9c4250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911 pos accuracy on test set\n"
     ]
    }
   ],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance','science_fiction'])\n",
    "brown_tagger = train_tagger(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45e1478a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fulton', 'NP'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('investigation', 'NN'),\n",
       " (\"Atlanta's\", 'NN'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'JJ'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBN'),\n",
       " ('evidence', 'NN'),\n",
       " ('irregularities', 'NNS'),\n",
       " ('took', 'VBD'),\n",
       " ('place', 'NN')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_tagger.tag(globals()[F'normalized_sentences_{opt}'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77129289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fulton', 'NNP'),\n",
       " ('County', 'NNP'),\n",
       " ('Grand', 'NNP'),\n",
       " ('Jury', 'NNP'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NNP'),\n",
       " ('investigation', 'NN'),\n",
       " (\"Atlanta's\", 'NNP'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'JJ'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBD'),\n",
       " ('evidence', 'NN'),\n",
       " ('irregularities', 'NNS'),\n",
       " ('took', 'VBD'),\n",
       " ('place', 'NN')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(globals()[F'normalized_sentences_{opt}'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dba843f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 12.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "dataframe_nonstop_lemstem_advjj = {}\n",
    "\n",
    "for opt in tqdm.tqdm(['religion', 'news']):\n",
    "    selected = []\n",
    "    for sentence in globals()[F'normalized_sentences_{opt}']:\n",
    "        tagged_sentence = brown_tagger.tag(sentence)\n",
    "        for w, pos in tagged_sentence:\n",
    "            if pos=='JJ':\n",
    "                selected.append(w) \n",
    "    dataframe_nonstop_lemstem_advjj[opt]= list(FreqDist(selected).most_common(40))\n",
    "\n",
    "dataframe_nonstop_lemstem_advjj = pd.DataFrame(dataframe_nonstop_lemstem_advjj)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d3435c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>religion</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(new, 77)</td>\n",
       "      <td>(new, 148)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Christian, 48)</td>\n",
       "      <td>(American, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(human, 43)</td>\n",
       "      <td>(high, 52)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Catholic, 33)</td>\n",
       "      <td>(public, 50)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(good, 29)</td>\n",
       "      <td>(good, 49)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(religious, 27)</td>\n",
       "      <td>(special, 41)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(great, 26)</td>\n",
       "      <td>(local, 39)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(social, 26)</td>\n",
       "      <td>(big, 38)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(spiritual, 25)</td>\n",
       "      <td>(federal, 35)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(moral, 21)</td>\n",
       "      <td>(young, 35)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(real, 20)</td>\n",
       "      <td>(long, 34)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(modern, 18)</td>\n",
       "      <td>(annual, 33)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(Chinese, 18)</td>\n",
       "      <td>(national, 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(important, 17)</td>\n",
       "      <td>(military, 30)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(American, 17)</td>\n",
       "      <td>(great, 30)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(certain, 17)</td>\n",
       "      <td>(foreign, 29)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(whole, 15)</td>\n",
       "      <td>(possible, 28)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(possible, 15)</td>\n",
       "      <td>(political, 27)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(political, 15)</td>\n",
       "      <td>(small, 27)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(evil, 14)</td>\n",
       "      <td>(open, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(Protestant, 14)</td>\n",
       "      <td>(major, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(personal, 14)</td>\n",
       "      <td>(large, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(natural, 14)</td>\n",
       "      <td>(able, 24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(magic, 14)</td>\n",
       "      <td>(British, 24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(traditional, 13)</td>\n",
       "      <td>(old, 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(English, 13)</td>\n",
       "      <td>(international, 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(small, 13)</td>\n",
       "      <td>(final, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(true, 12)</td>\n",
       "      <td>(Catholic, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(Roman, 12)</td>\n",
       "      <td>(anti-trust, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(entire, 12)</td>\n",
       "      <td>(recent, 20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(theological, 11)</td>\n",
       "      <td>(private, 19)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(public, 11)</td>\n",
       "      <td>(medical, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(anti-slavery, 11)</td>\n",
       "      <td>(certain, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(large, 10)</td>\n",
       "      <td>(complete, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(individual, 10)</td>\n",
       "      <td>(necessary, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(recent, 10)</td>\n",
       "      <td>(full, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(professional, 10)</td>\n",
       "      <td>(similar, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(nuclear, 10)</td>\n",
       "      <td>(real, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(common, 9)</td>\n",
       "      <td>(social, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(popular, 9)</td>\n",
       "      <td>(fine, 16)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              religion                 news\n",
       "0            (new, 77)           (new, 148)\n",
       "1      (Christian, 48)       (American, 66)\n",
       "2          (human, 43)           (high, 52)\n",
       "3       (Catholic, 33)         (public, 50)\n",
       "4           (good, 29)           (good, 49)\n",
       "5      (religious, 27)        (special, 41)\n",
       "6          (great, 26)          (local, 39)\n",
       "7         (social, 26)            (big, 38)\n",
       "8      (spiritual, 25)        (federal, 35)\n",
       "9          (moral, 21)          (young, 35)\n",
       "10          (real, 20)           (long, 34)\n",
       "11        (modern, 18)         (annual, 33)\n",
       "12       (Chinese, 18)       (national, 32)\n",
       "13     (important, 17)       (military, 30)\n",
       "14      (American, 17)          (great, 30)\n",
       "15       (certain, 17)        (foreign, 29)\n",
       "16         (whole, 15)       (possible, 28)\n",
       "17      (possible, 15)      (political, 27)\n",
       "18     (political, 15)          (small, 27)\n",
       "19          (evil, 14)           (open, 26)\n",
       "20    (Protestant, 14)          (major, 25)\n",
       "21      (personal, 14)          (large, 25)\n",
       "22       (natural, 14)           (able, 24)\n",
       "23         (magic, 14)        (British, 24)\n",
       "24   (traditional, 13)            (old, 23)\n",
       "25       (English, 13)  (international, 23)\n",
       "26         (small, 13)          (final, 21)\n",
       "27          (true, 12)       (Catholic, 21)\n",
       "28         (Roman, 12)     (anti-trust, 21)\n",
       "29        (entire, 12)         (recent, 20)\n",
       "30   (theological, 11)        (private, 19)\n",
       "31        (public, 11)        (medical, 18)\n",
       "32  (anti-slavery, 11)        (certain, 18)\n",
       "33         (large, 10)       (complete, 18)\n",
       "34    (individual, 10)      (necessary, 18)\n",
       "35        (recent, 10)           (full, 18)\n",
       "36  (professional, 10)        (similar, 17)\n",
       "37       (nuclear, 10)           (real, 17)\n",
       "38         (common, 9)         (social, 17)\n",
       "39        (popular, 9)           (fine, 16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_nonstop_lemstem_advjj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36859470",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § دیدن سیاق  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73901a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 25 matches:\n",
      " spirit matter term generally taken granted \n",
      "cisely one must modern world neither spirit matter refer generally agreed-upon e\n",
      "t almost universal disbelief reality spirit century ago world spirit populous re\n",
      "ief reality spirit century ago world spirit populous real world material entity \n",
      "man rock tree star world word matter spirit referred directly known reality comm\n",
      "ategory reality large extent emptied spirit world entity previously populated ca\n",
      "aim due credit objectification world spirit popular superstition certainly gone \n",
      "certainly gone far beyond experience spirit could justify support Science fully \n",
      "ty popular imagination peopled world spirit entity soon lost whatever status enj\n",
      "nmixed blessing scientific debunking spirit world way successful thorough house \n",
      "y inadequate mean dealing experience spirit Although particular form conceptuali\n",
      "imagination made response experience spirit undoubtedly defective raw experience\n",
      " leaf play spell kind spell exposure spirit living active manifestation always e\n",
      "in terrible power better answer give spirit attempt say spirit employ commonly u\n",
      "etter answer give spirit attempt say spirit employ commonly used word designate \n",
      " mind Hale's still convinced society spirit beyond ken page hand little later sa\n",
      "s come watch play like spell reality spirit emerges play spite author's convicti\n",
      "y spite author's conviction contrary Spirit community nothing whole range human \n",
      "rience widely known universally felt spirit Apart spirit could community spirit \n",
      " known universally felt spirit Apart spirit could community spirit draw men comm\n",
      " spirit Apart spirit could community spirit draw men community give community un\n",
      "ohesiveness permanence Think example spirit Marine Corps Surely reality acknowle\n",
      "ctive reality experienced known many spirit know spirit Nazism Communism school \n",
      "y experienced known many spirit know spirit Nazism Communism school spirit spiri\n",
      " know spirit Nazism Communism school spirit spirit street corner gang football t\n"
     ]
    }
   ],
   "source": [
    "hafez_text = nltk.Text(religion_tokens_nonstop_lemstem)\n",
    "hafez_text.concordance('Spirit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec10a0",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § عبارت یابی  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cef5df48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', '1801-Jefferson.txt', '1805-Jefferson.txt', '1809-Madison.txt', '1813-Madison.txt', '1817-Monroe.txt', '1821-Monroe.txt', '1825-Adams.txt', '1829-Jackson.txt', '1833-Jackson.txt', '1837-VanBuren.txt', '1841-Harrison.txt', '1845-Polk.txt', '1849-Taylor.txt', '1853-Pierce.txt', '1857-Buchanan.txt', '1861-Lincoln.txt', '1865-Lincoln.txt', '1869-Grant.txt', '1873-Grant.txt', '1877-Hayes.txt', '1881-Garfield.txt', '1885-Cleveland.txt', '1889-Harrison.txt', '1893-Cleveland.txt', '1897-McKinley.txt', '1901-McKinley.txt', '1905-Roosevelt.txt', '1909-Taft.txt', '1913-Wilson.txt', '1917-Wilson.txt', '1921-Harding.txt', '1925-Coolidge.txt', '1929-Hoover.txt', '1933-Roosevelt.txt', '1937-Roosevelt.txt', '1941-Roosevelt.txt', '1945-Roosevelt.txt', '1949-Truman.txt', '1953-Eisenhower.txt', '1957-Eisenhower.txt', '1961-Kennedy.txt', '1965-Johnson.txt', '1969-Nixon.txt', '1973-Nixon.txt', '1977-Carter.txt', '1981-Reagan.txt', '1985-Reagan.txt', '1989-Bush.txt', '1993-Clinton.txt', '1997-Clinton.txt', '2001-Bush.txt', '2005-Bush.txt', '2009-Obama.txt', '2013-Obama.txt', '2017-Trump.txt']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "\n",
    "print(inaugural.fileids())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9205c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lincoln = list(inaugural.words(inaugural.fileids()[18]))\n",
    "Obama = list(inaugural.words(inaugural.fileids()[56]))\n",
    "Trump = list(inaugural.words(inaugural.fileids()[57]))\n",
    "\n",
    "Lincoln =' '.join(Lincoln)\n",
    "Lincoln_sentences =  sent_tokenize(Lincoln)\n",
    "Lincoln_sentences =  [x.split() for x in Lincoln_sentences]\n",
    "Lincoln_sentences_tagged = [brown_tagger.tag(sent) for sent in Lincoln_sentences]\n",
    "\n",
    "Obama =' '.join(Obama)\n",
    "Obama_sentences =  sent_tokenize(Obama)\n",
    "Obama_sentences =  [x.split() for x in Obama_sentences]\n",
    "Obama_sentences_tagged = [brown_tagger.tag(sent) for sent in Obama_sentences]\n",
    "\n",
    "Trump =' '.join(Trump)\n",
    "Trump_sentences =  sent_tokenize(Trump)\n",
    "Trump_sentences =  [x.split() for x in Trump_sentences]\n",
    "Trump_sentences_tagged = [brown_tagger.tag(sent) for sent in Trump_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0f83542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words  4005            \n",
      "Number of unique words 1078            \n",
      "Average word length 4.329588014981273\n",
      "Average sentence length in characters 7.659176029962547\n",
      "Longest word     unconstitutional\n"
     ]
    }
   ],
   "source": [
    "sentence_words = list(itertools.chain(*Lincoln_sentences))\n",
    "print ('%-16s' % 'Number of words', '%-16s' % len(sentence_words))\n",
    "print ('%-16s' % 'Number of unique words', '%-16s' % len(set(sentence_words)))\n",
    "avg=np.sum([len(word) for word in sentence_words])/len(sentence_words)\n",
    "print ('%-16s' % 'Average word length', '%-16s' % avg)\n",
    "avg_sentence_length_c=np.mean([len(' '.join(sentence)) for sentence in sentence_words])\n",
    "print ('%-16s' % 'Average sentence length in characters', '%-16s' % avg_sentence_length_c)\n",
    "print ('%-16s' % 'Longest word', '%-16s' % sentence_words[np.argmax([len(word) for word in sentence_words])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab2cb930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words  2369            \n",
      "Number of unique words 820             \n",
      "Average word length 4.149430139299282\n",
      "Average sentence length in characters 7.298860278598565\n",
      "Longest word     responsibility  \n"
     ]
    }
   ],
   "source": [
    "sentence_words = list(itertools.chain(*Obama_sentences))\n",
    "print ('%-16s' % 'Number of words', '%-16s' % len(sentence_words))\n",
    "print ('%-16s' % 'Number of unique words', '%-16s' % len(set(sentence_words)))\n",
    "avg=np.sum([len(word) for word in sentence_words])/len(sentence_words)\n",
    "print ('%-16s' % 'Average word length', '%-16s' % avg)\n",
    "avg_sentence_length_c=np.mean([len(' '.join(sentence)) for sentence in sentence_words])\n",
    "print ('%-16s' % 'Average sentence length in characters', '%-16s' % avg_sentence_length_c)\n",
    "print ('%-16s' % 'Longest word', '%-16s' % sentence_words[np.argmax([len(word) for word in sentence_words])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad9de0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words  1693            \n",
      "Number of unique words 582             \n",
      "Average word length 4.112226816302422\n",
      "Average sentence length in characters 7.224453632604844\n",
      "Longest word     administration  \n"
     ]
    }
   ],
   "source": [
    "sentence_words = list(itertools.chain(*Trump_sentences))\n",
    "print ('%-16s' % 'Number of words', '%-16s' % len(sentence_words))\n",
    "print ('%-16s' % 'Number of unique words', '%-16s' % len(set(sentence_words)))\n",
    "avg=np.sum([len(word) for word in sentence_words])/len(sentence_words)\n",
    "print ('%-16s' % 'Average word length', '%-16s' % avg)\n",
    "avg_sentence_length_c=np.mean([len(' '.join(sentence)) for sentence in sentence_words])\n",
    "print ('%-16s' % 'Average sentence length in characters', '%-16s' % avg_sentence_length_c)\n",
    "print ('%-16s' % 'Longest word', '%-16s' % sentence_words[np.argmax([len(word) for word in sentence_words])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87ccf1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkTechnicalTerm(sentence):\n",
    "    grammar = r\"\"\"\n",
    "      TECHTERM: {<JJ|NN>+<NN|CD>|<NN>}\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    return (cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7165f046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 61.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "technical_terms = {}\n",
    "opt_technical_terms = {}\n",
    "\n",
    "\n",
    "\n",
    "for opt in tqdm.tqdm(['Lincoln', 'Obama','Trump']):\n",
    "    technical_terms[opt] = []\n",
    "    tagged = globals()[F'{opt}_sentences_tagged']\n",
    "    for sentence in tagged:\n",
    "        tree=chunkTechnicalTerm(sentence)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'TECHTERM':\n",
    "                technical_terms[opt].append(subtree.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "699d1ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1448.64it/s]\n"
     ]
    }
   ],
   "source": [
    "for opt in tqdm.tqdm(['Lincoln', 'Obama','Trump']):\n",
    "    opt_technical_terms[opt]=FreqDist([' '.join([x for x,y in sent]) for sent in technical_terms[opt] if len(sent)>1]).most_common(40)    \n",
    "                                   \n",
    "                                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c772fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lincoln</th>\n",
       "      <th>Obama</th>\n",
       "      <th>Trump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(such service, 2)</td>\n",
       "      <td>(exceptionalâ , 1)</td>\n",
       "      <td>(transferring power, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(organic law, 2)</td>\n",
       "      <td>(Americanâ , 1)</td>\n",
       "      <td>(great national effort, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(constitutional right, 2)</td>\n",
       "      <td>(modern economy, 1)</td>\n",
       "      <td>(peaceful transfer, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(foreign slave trade, 2)</td>\n",
       "      <td>(free market, 1)</td>\n",
       "      <td>(Michelle Obama, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(whole subject, 2)</td>\n",
       "      <td>(fair play, 1)</td>\n",
       "      <td>(gracious aid, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(office .\", 1)</td>\n",
       "      <td>(great nation, 1)</td>\n",
       "      <td>(s ceremony, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(special anxiety, 1)</td>\n",
       "      <td>(central authority, 1)</td>\n",
       "      <td>(special meaning, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(personal security, 1)</td>\n",
       "      <td>(hard work, 1)</td>\n",
       "      <td>(small group, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(reasonable cause, 1)</td>\n",
       "      <td>(personal responsibility, 1)</td>\n",
       "      <td>(historic movement, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(such apprehension, 1)</td>\n",
       "      <td>(collective action, 1)</td>\n",
       "      <td>(crucial conviction, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(ample evidence, 1)</td>\n",
       "      <td>(s world, 1)</td>\n",
       "      <td>(different reality, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(lawful right, 1)</td>\n",
       "      <td>(research labs, 1)</td>\n",
       "      <td>(education system, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(full knowledge, 1)</td>\n",
       "      <td>(economic recovery, 1)</td>\n",
       "      <td>(unrealized potential, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(emphatic resolution, 1)</td>\n",
       "      <td>(endless capacity, 1)</td>\n",
       "      <td>(glorious destiny, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(own judgment, 1)</td>\n",
       "      <td>(itâ , 1)</td>\n",
       "      <td>(foreign industry, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(political fabric, 1)</td>\n",
       "      <td>(s prosperity, 1)</td>\n",
       "      <td>(American industry, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(lawless invasion, 1)</td>\n",
       "      <td>(middle class, 1)</td>\n",
       "      <td>(sad depletion, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(public attention, 1)</td>\n",
       "      <td>(honest labor, 1)</td>\n",
       "      <td>(s infrastructure, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(conclusive evidence, 1)</td>\n",
       "      <td>(little girl, 1)</td>\n",
       "      <td>(rich while, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(clause \", 1)</td>\n",
       "      <td>(bleakest poverty, 1)</td>\n",
       "      <td>(middle class, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(good temper, 1)</td>\n",
       "      <td>(real meaning, 1)</td>\n",
       "      <td>(new decree, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(equal unanimity frame, 1)</td>\n",
       "      <td>(basic measure, 1)</td>\n",
       "      <td>(foreign capital, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(unanimous oath, 1)</td>\n",
       "      <td>(health care, 1)</td>\n",
       "      <td>(new vision, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(material one, 1)</td>\n",
       "      <td>(country freedom, 1)</td>\n",
       "      <td>(great prosperity, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(little consequence, 1)</td>\n",
       "      <td>(job loss, 1)</td>\n",
       "      <td>(American labor, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(unsubstantial controversy, 1)</td>\n",
       "      <td>(sudden illness, 1)</td>\n",
       "      <td>(good will, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(humane jurisprudence, 1)</td>\n",
       "      <td>(terrible storm, 1)</td>\n",
       "      <td>(exampleâ , 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(free man, 1)</td>\n",
       "      <td>(overwhelming judgment, 1)</td>\n",
       "      <td>(shineâ , 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(official oath, 1)</td>\n",
       "      <td>(sustainable energy, 1)</td>\n",
       "      <td>(radical Islamic terrorism, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(period fifteen, 1)</td>\n",
       "      <td>(economic vitality, 1)</td>\n",
       "      <td>(total allegiance, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(executive branch, 1)</td>\n",
       "      <td>(national treasureâ , 1)</td>\n",
       "      <td>(unity .\", 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(great success, 1)</td>\n",
       "      <td>(perpetual war, 1)</td>\n",
       "      <td>(law enforcement, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(brief constitutional term, 1)</td>\n",
       "      <td>(friendsâ , 1)</td>\n",
       "      <td>(empty talk, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(peculiar difficulty, 1)</td>\n",
       "      <td>(peacefullyâ , 1)</td>\n",
       "      <td>(new millennium, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(universal law, 1)</td>\n",
       "      <td>(peaceful world, 1)</td>\n",
       "      <td>(new national pride, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(fundamental law, 1)</td>\n",
       "      <td>(powerful nation, 1)</td>\n",
       "      <td>(s time, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(own termination, 1)</td>\n",
       "      <td>(prejudiceâ , 1)</td>\n",
       "      <td>(old wisdom, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(peaceably unmade, 1)</td>\n",
       "      <td>(mere charity, 1)</td>\n",
       "      <td>(red blood, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(lawfully rescind, 1)</td>\n",
       "      <td>(constant advance, 1)</td>\n",
       "      <td>(great American flag, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(legal contemplation, 1)</td>\n",
       "      <td>(common creed, 1)</td>\n",
       "      <td>(urban sprawl, 1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Lincoln                         Obama  \\\n",
       "0                (such service, 2)          (exceptionalâ , 1)   \n",
       "1                 (organic law, 2)             (Americanâ , 1)   \n",
       "2        (constitutional right, 2)           (modern economy, 1)   \n",
       "3         (foreign slave trade, 2)              (free market, 1)   \n",
       "4               (whole subject, 2)                (fair play, 1)   \n",
       "5                   (office .\", 1)             (great nation, 1)   \n",
       "6             (special anxiety, 1)        (central authority, 1)   \n",
       "7           (personal security, 1)                (hard work, 1)   \n",
       "8            (reasonable cause, 1)  (personal responsibility, 1)   \n",
       "9           (such apprehension, 1)        (collective action, 1)   \n",
       "10             (ample evidence, 1)                  (s world, 1)   \n",
       "11               (lawful right, 1)            (research labs, 1)   \n",
       "12             (full knowledge, 1)        (economic recovery, 1)   \n",
       "13        (emphatic resolution, 1)         (endless capacity, 1)   \n",
       "14               (own judgment, 1)                   (itâ , 1)   \n",
       "15           (political fabric, 1)             (s prosperity, 1)   \n",
       "16           (lawless invasion, 1)             (middle class, 1)   \n",
       "17           (public attention, 1)             (honest labor, 1)   \n",
       "18        (conclusive evidence, 1)              (little girl, 1)   \n",
       "19                   (clause \", 1)         (bleakest poverty, 1)   \n",
       "20                (good temper, 1)             (real meaning, 1)   \n",
       "21      (equal unanimity frame, 1)            (basic measure, 1)   \n",
       "22             (unanimous oath, 1)              (health care, 1)   \n",
       "23               (material one, 1)          (country freedom, 1)   \n",
       "24         (little consequence, 1)                 (job loss, 1)   \n",
       "25  (unsubstantial controversy, 1)           (sudden illness, 1)   \n",
       "26       (humane jurisprudence, 1)           (terrible storm, 1)   \n",
       "27                   (free man, 1)    (overwhelming judgment, 1)   \n",
       "28              (official oath, 1)       (sustainable energy, 1)   \n",
       "29             (period fifteen, 1)        (economic vitality, 1)   \n",
       "30           (executive branch, 1)    (national treasureâ , 1)   \n",
       "31              (great success, 1)            (perpetual war, 1)   \n",
       "32  (brief constitutional term, 1)              (friendsâ , 1)   \n",
       "33        (peculiar difficulty, 1)           (peacefullyâ , 1)   \n",
       "34              (universal law, 1)           (peaceful world, 1)   \n",
       "35            (fundamental law, 1)          (powerful nation, 1)   \n",
       "36            (own termination, 1)            (prejudiceâ , 1)   \n",
       "37           (peaceably unmade, 1)             (mere charity, 1)   \n",
       "38           (lawfully rescind, 1)         (constant advance, 1)   \n",
       "39        (legal contemplation, 1)             (common creed, 1)   \n",
       "\n",
       "                             Trump  \n",
       "0          (transferring power, 2)  \n",
       "1       (great national effort, 1)  \n",
       "2           (peaceful transfer, 1)  \n",
       "3              (Michelle Obama, 1)  \n",
       "4                (gracious aid, 1)  \n",
       "5                  (s ceremony, 1)  \n",
       "6             (special meaning, 1)  \n",
       "7                 (small group, 1)  \n",
       "8           (historic movement, 1)  \n",
       "9          (crucial conviction, 1)  \n",
       "10          (different reality, 1)  \n",
       "11           (education system, 1)  \n",
       "12       (unrealized potential, 1)  \n",
       "13           (glorious destiny, 1)  \n",
       "14           (foreign industry, 1)  \n",
       "15          (American industry, 1)  \n",
       "16              (sad depletion, 1)  \n",
       "17           (s infrastructure, 1)  \n",
       "18                 (rich while, 1)  \n",
       "19               (middle class, 1)  \n",
       "20                 (new decree, 1)  \n",
       "21            (foreign capital, 1)  \n",
       "22                 (new vision, 1)  \n",
       "23           (great prosperity, 1)  \n",
       "24             (American labor, 1)  \n",
       "25                  (good will, 1)  \n",
       "26                (exampleâ , 1)  \n",
       "27                  (shineâ , 1)  \n",
       "28  (radical Islamic terrorism, 1)  \n",
       "29           (total allegiance, 1)  \n",
       "30                   (unity .\", 1)  \n",
       "31            (law enforcement, 1)  \n",
       "32                 (empty talk, 1)  \n",
       "33             (new millennium, 1)  \n",
       "34         (new national pride, 1)  \n",
       "35                     (s time, 1)  \n",
       "36                 (old wisdom, 1)  \n",
       "37                  (red blood, 1)  \n",
       "38        (great American flag, 1)  \n",
       "39               (urban sprawl, 1)  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_technical_terms = pd.DataFrame(opt_technical_terms) \n",
    "opt_technical_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18620d1",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § WordNet  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b7db689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "from string import *\n",
    "\n",
    "def freq_normed_unigrams(sents):\n",
    "    wnl = WordNetLemmatizer() # to get word stems\n",
    "    \n",
    "    tagged_POS_sents = [nltk.pos_tag(sent) for sent in sents] # tags sents\n",
    "    \n",
    "    normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           for word in sent \n",
    "                           if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           and word[0] not in punctuation # remove punctuation\n",
    "                           and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           and word[1].startswith('N')]  # include only nouns\n",
    "\n",
    "    top_normed_unigrams = [word for (word, count) in nltk.FreqDist(normed_tagged_words).most_common(40)]\n",
    "    return top_normed_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67ace76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trump_top_words = freq_normed_unigrams(Trump_sentences)\n",
    "Obama_top_words = freq_normed_unigrams(Obama_sentences)\n",
    "Lincoln_top_words = freq_normed_unigrams(Lincoln_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "457a7c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "\n",
    "def categories_from_hypernyms(termlist):\n",
    "    hypterms = []\n",
    "    hypterms_dict = defaultdict()\n",
    "    for term in termlist:                  # for each term\n",
    "        s = wn.synsets(term.lower(), 'n')  # get its nominal synsets\n",
    "        for syn in s:                      # for each lemma synset\n",
    "            for hyp in syn.hypernyms():    # It has a list of hypernyms\n",
    "                hypterms.append(hyp.name())      # Extract the hypernym name and add to list\n",
    "                if hyp.name() not in hypterms_dict:\n",
    "                    hypterms_dict[hyp.name()] = list()\n",
    "                hypterms_dict[hyp.name()].append(term)  # Extract examples and add them to dict\n",
    "                \n",
    "    hypfd = nltk.FreqDist(hypterms)             # After going through all the nouns, print out the hypernyms \n",
    "    for (name, count) in hypfd.most_common(25):  # that have accumulated the most counts (have seen the most descendents)\n",
    "        print( name, '({0})'.format(count))\n",
    "        print ('\\t', ', '.join(set(hypterms_dict[name])))  # show the children found for each hypernym\n",
    "        print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87664090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_period.n.01 (8)\n",
      "\t day, year, life\n",
      "\n",
      "people.n.01 (4)\n",
      "\t world, land, nation, country\n",
      "\n",
      "group.n.01 (4)\n",
      "\t world, people\n",
      "\n",
      "political_unit.n.01 (3)\n",
      "\t land, nation, country\n",
      "\n",
      "administrative_district.n.01 (3)\n",
      "\t city, land, country\n",
      "\n",
      "assets.n.01 (3)\n",
      "\t capital, share\n",
      "\n",
      "person.n.01 (3)\n",
      "\t life, child, party\n",
      "\n",
      "being.n.01 (3)\n",
      "\t life\n",
      "\n",
      "region.n.03 (2)\n",
      "\t land, country\n",
      "\n",
      "inhabitant.n.01 (2)\n",
      "\t american\n",
      "\n",
      "experience.n.02 (2)\n",
      "\t world, life\n",
      "\n",
      "head_of_state.n.01 (2)\n",
      "\t president\n",
      "\n",
      "work.n.01 (2)\n",
      "\t job\n",
      "\n",
      "time_unit.n.01 (2)\n",
      "\t day\n",
      "\n",
      "boundary.n.01 (2)\n",
      "\t border\n",
      "\n",
      "edge.n.06 (2)\n",
      "\t border\n",
      "\n",
      "leader.n.01 (2)\n",
      "\t politician\n",
      "\n",
      "ending.n.04 (2)\n",
      "\t victory, triumph\n",
      "\n",
      "success.n.01 (2)\n",
      "\t victory, triumph\n",
      "\n",
      "real_property.n.01 (2)\n",
      "\t land\n",
      "\n",
      "object.n.01 (2)\n",
      "\t land\n",
      "\n",
      "geographical_area.n.01 (1)\n",
      "\t country\n",
      "\n",
      "confederation.n.02 (1)\n",
      "\t nation\n",
      "\n",
      "family.n.04 (1)\n",
      "\t people\n",
      "\n",
      "english.n.01 (1)\n",
      "\t american\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories_from_hypernyms(Trump_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2ad0d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_period.n.01 (9)\n",
      "\t year, time, generation, life\n",
      "\n",
      "group.n.01 (5)\n",
      "\t world, people, men\n",
      "\n",
      "people.n.01 (4)\n",
      "\t world, generation, nation, country\n",
      "\n",
      "being.n.01 (3)\n",
      "\t life\n",
      "\n",
      "person.n.01 (3)\n",
      "\t life, child, men\n",
      "\n",
      "male.n.02 (3)\n",
      "\t men\n",
      "\n",
      "direction.n.06 (3)\n",
      "\t rule\n",
      "\n",
      "activity.n.01 (3)\n",
      "\t job, effort, work\n",
      "\n",
      "political_unit.n.01 (2)\n",
      "\t nation, country\n",
      "\n",
      "inhabitant.n.01 (2)\n",
      "\t american\n",
      "\n",
      "experience.n.02 (2)\n",
      "\t world, life\n",
      "\n",
      "freedom.n.01 (2)\n",
      "\t liberty\n",
      "\n",
      "doctrine.n.01 (2)\n",
      "\t creed\n",
      "\n",
      "promise.n.01 (2)\n",
      "\t oath, word\n",
      "\n",
      "generalization.n.02 (2)\n",
      "\t rule, principle\n",
      "\n",
      "law.n.04 (2)\n",
      "\t rule, principle\n",
      "\n",
      "homo.n.02 (2)\n",
      "\t world, men\n",
      "\n",
      "concept.n.01 (2)\n",
      "\t rule\n",
      "\n",
      "duration.n.01 (2)\n",
      "\t value, rule\n",
      "\n",
      "product.n.02 (2)\n",
      "\t job, work\n",
      "\n",
      "work.n.01 (2)\n",
      "\t job\n",
      "\n",
      "head_of_state.n.01 (2)\n",
      "\t president\n",
      "\n",
      "family.n.04 (1)\n",
      "\t people\n",
      "\n",
      "case.n.01 (1)\n",
      "\t time\n",
      "\n",
      "moment.n.01 (1)\n",
      "\t time\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories_from_hypernyms(Obama_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d58f3381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person.n.01 (6)\n",
      "\t slave, case, authority, party\n",
      "\n",
      "time_period.n.01 (6)\n",
      "\t term, time, year\n",
      "\n",
      "concept.n.01 (4)\n",
      "\t section, part, law\n",
      "\n",
      "work.n.01 (4)\n",
      "\t labor, duty, service\n",
      "\n",
      "group.n.01 (3)\n",
      "\t people\n",
      "\n",
      "organization.n.01 (3)\n",
      "\t union, institution, party\n",
      "\n",
      "state.n.02 (3)\n",
      "\t union, office, power\n",
      "\n",
      "social_control.n.01 (3)\n",
      "\t duty, administration, government\n",
      "\n",
      "administrative_district.n.01 (2)\n",
      "\t state\n",
      "\n",
      "attribute.n.02 (2)\n",
      "\t state, time\n",
      "\n",
      "political_unit.n.01 (2)\n",
      "\t union, state\n",
      "\n",
      "executive_department.n.01 (2)\n",
      "\t labor, state\n",
      "\n",
      "beginning.n.05 (2)\n",
      "\t institution, constitution\n",
      "\n",
      "collection.n.01 (2)\n",
      "\t hand, law\n",
      "\n",
      "force.n.04 (2)\n",
      "\t service, law\n",
      "\n",
      "sexual_activity.n.01 (2)\n",
      "\t union, congress\n",
      "\n",
      "happening.n.01 (2)\n",
      "\t union, case\n",
      "\n",
      "container.n.01 (2)\n",
      "\t case\n",
      "\n",
      "activity.n.01 (2)\n",
      "\t service, provision\n",
      "\n",
      "number.n.01 (2)\n",
      "\t majority, minority\n",
      "\n",
      "age.n.03 (2)\n",
      "\t majority, minority\n",
      "\n",
      "administrative_unit.n.01 (2)\n",
      "\t office, authority\n",
      "\n",
      "goal.n.01 (2)\n",
      "\t purpose, object\n",
      "\n",
      "constituent.n.04 (2)\n",
      "\t term, object\n",
      "\n",
      "resoluteness.n.01 (2)\n",
      "\t purpose, decision\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories_from_hypernyms(Lincoln_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe523ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
